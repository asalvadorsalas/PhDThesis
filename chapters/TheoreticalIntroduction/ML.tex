\chapter{Machine learning and statistical methods}
\label{chapter:MLStat}

In order to test the predictions of a given model, experimental data and \acrshort{MClabel} simulations are compared using statistical methods. This chapter describes the tools used to extract a measurement of the production of the target signal, in addition to upper limits.

\acrlong{MLlabel}~(\acrshort{MClabel}) is one of the core developing fields in computer science allowing the analysis of large and complex datasets, offering sophisticated techniques with a broad range of possible applications. Regarding high energy physics, the large amount of \acrshort{MClabel} simulations or data that is being recorded is well suited for the application of \acrshort{MLlabel} techniques. In this chapter, different multi-variate techniques used in this thesis are introduced, focusing on the classification methods used to improve signal and background separation.

\section{Profile likelihood fit}

In order to test the compatibility between data and the \acrshort{MClabel} simulations, statistical methods in the context of hypothesis testing need to be introduced. The profile likelihood fit is a statistical tool used in this thesis to extract a measurement for the production of the signal targetted in the analysis. Also, the upper limits are extracted based on the asymptotic formulation. In this Section, the profile likelihood fit method is presented with the necessary concepts in the context of a \acrshort{BSMlabel} search. %https://arxiv.org/abs/1007.1727
The technical implementation is provided by the RooStat framework. %https://arxiv.org/abs/1009.1003

The idea behind hypothesis testing is to compare the agreement of the experimental data between two hypotheses and quantify which hypothesis can be discarded at a certain level of confidence. The two hypotheses to be compared are: the null-hypothesis $H_0$, the~\acrshort{SMlabel} without new physics, and the alternative hypothesis $H_\mu$ which accounts for \acrshort{BSMlabel} interactions. The $\mu$ refers to the signal strength, commonly referred as parameter of interest (POI), which is a normalisation factor for the targetted signal that can be expressed as,

\begin{equation}
    \mu = \frac{\sigma}{\sigma_{ref}}
\end{equation}

where $\sigma$ is arbitrary and $\sigma_{ref}$ a reference value, typically a benchmark value from a theory or an expected sensitivity like 1~pb. For the statistical method, the agreement is checked for $H_\mu$ with a continuous spectrum of signal strengths, which will approach the \acrshort{SMlabel}, $H_0$, for $\mu=0$.

Given a binned data distribution with $n_i$ events for a bin $i$, the expected events can be expressed as,

\begin{equation}
    E[n_i(\mu,\mathbf{b},\boldsymbol{\theta})] = \mu\cdot s_i(\boldsymbol{\theta}) + \sum_{k_{\alpha}\in\mathbf{k}}k_\alpha\cdot b_{\alpha,i}(\boldsymbol{\theta})
\end{equation}

with $s_i$ the predicted signal events, $b_{\alpha,i}$ the predicted background events of the process $\alpha$. The normalisation factor $k_\alpha$ allows the background process $\alpha$ to vary in the fit. In this thesis, only two backgrounds are allowed to have freely floating normalisation factors, which are determined in the fit to data. The rest of the processes are normalised to their predicted cross-sections and the corresponding $k_\alpha$ is fixed to one. The nuisance parameters $\boldsymbol{\theta}$ are additional degrees of freedom which correspond to the systematic uncertainties acting both on the shape and normalisation of all processes. Their central value is defined to be zero and the deviation with respect to the original value is refered to as pull, where a deviation of $\pm$1 corresponds to a variation of one standard deviation. The fit procedure allows the reduction of the impact of systematic uncertainties, especially by taking advantage of the highly populated background-dominated bins included in the fit. This requires a good understanding of the background and the systematic effects. To verify the improved background prediction, fits under the background-only hypothesis are typically performed, and differences between the data and the post-fit background prediction are checked using kinematic variables other than the ones used in the fit.

The binned likelihood function is given as
\begin{equation}
    \mathscr{L}(\mu,\mathbf{k},\boldsymbol{\theta}) = \prod_i^N \frac{ (E[n_i(\mu,\mathbf{b},\boldsymbol{\theta})])^{n_i}}{n_i!}e^{E[n_i(\mu,\mathbf{b},\boldsymbol{\theta})]}\prod_{\theta_j\in\boldsymbol{\theta}}P(\theta_j)
\end{equation}

which corresponds to a product of Poisson probabilities for all $N$ bins and the penalty terms of all nuisance parameters. The $P(\theta_j)$ are generally Gaussian distributions for systematic uncertainties and Poisson for the statistical uncertanty of each bin, that are introduced in the likelihood to penalise large deviations.
The optimal $\mu$, $\mathbf{k}$ and $\boldsymbol{\theta}$ are obtained from the fit to data that maximises the agreement between data and the prediction.

The optimal test statistic to perform the fit is the likelihood ratio, %https://link.springer.com/chapter/10.1007/978-1-4612-0919-5_5

\begin{equation}
    \lambda_\mu = \frac{\mathscr{L}(\mu, \hat{\hat{\mathbf{k}}},\hat{\hat{\mathbf{k}}})}{\mathscr{L}(\hat{\mu}, \hat{\mathbf{k}},\hat{hat{\mathbf{k}}})}
\end{equation}

with the single-hat parameters that maximise likelihood while $\hat{\mathbf{k}},\hat{hat{\mathbf{k}}}$ those that maximise the likelihood for a given $\mu$. As the likelihoods are products of several terms smaller than one, a more stable test statistic is the negative log-likelihood,

\begin{equation}
    q_\mu = -2\ln\lambda_\mu
\end{equation}

For the purpose of setting upper limits on the signal production, some special cases are defined depending on $\mu$ and $\hat{\mu}$. If $\hat{\mu}$ is negative, i.e. the fitted signal has a negative normalisation, the modified test statistic assumes signal to be only positive: $\tilde{q}(\mu)=-2\ln\frac{\mathscr{L}(\mu, \hat{\hat{\mathbf{k}}},\hat{\hat{\mathbf{k}}})}{\mathscr{L}(0, \hat{\hat{\mathbf{k}}},\hat{\hat{\mathbf{k}}})}$, where the parameters in the denominator optimise the likelihood for $\mu=0$. Another exception is to set the modified test statistic to 0 for $\hat{\mu}>\mu$, as signal below the observed is in complete agreement with the observed measurement.

The level of agreement between data and predictions for a given signal strength is quantified by computing the p-value, $p_\mu$, which is the probability of the measured data being a deviation from $H_\mu$, assumed true,

\begin{equation}
    p_\mu = \int_{q_{\mu,obs}}^\infty f(q_\mu|H_\mu)dq_\mu
\end{equation}

where $f(q_\mu|H_\mu)$ is the probability density function of $q_\mu$ under the assumption of $H_\mu$. The significance $Z=\Phi^{-1}(1-p_\mu)$ (being $\Phi$ the cumulative Gaussian distribution) is often preferred to quantify the level of disagreement in terms of sigma deviations. Typically, an alternative hypothesis is rejected at 1.64$\sigma$ ($p_\mu=0.05$) and the background-only at 5$\sigma$ ($p_0=2.87\cdot10^{-7}$).

Normally, searches are dedicated to very small signals and poorly separated from the background. Rejecting the null hypothesis at a fixed probability as mentioned, leads to exclude signals with very low statistics not really targetted by the analysis. %https://www.sciencedirect.com/science/article/abs/pii/S0168900299004982?via%3Dihub
The $CL_{s}$ method solves this issue,

\begin{equation}
    CL_{s}=\frac{p_\mu}{1-p_0}
\end{equation}

which is the previous p-value, which any bad compatibility benefits the background-only hypothesis, normalised to the confidence level of the background-only hypothesis, which when closer to 1 points that the measurement is not compatible with the background. In general, the exclusion limits obtained with this method are conservative.

\section{Machine Learning}

The deployment of \acrshort{MLlabel} methods is already reaching crucial tasks in \acrshort{ATLASlabel} as online data recording, from neural networks in calorimetry FPGAs  %https://inspirehep.net/literature/2013724
to particle reconstruction in trigger algorithms %https://inspirehep.net/literature/1795210
, which benefit from faster and more efficient response than previous filters. For those cases, a neural network is trained to reduce background signal, to offer a high-level discriminating variable for a classification problem or to provide a prediction of a certain quantity. These methods can outperform conventional algorithms as the inference is usually performed from multi-dimensional inputs, providing large amounts of information to the learning algorithm. Regarding simulation, the detector simulation is one of the most computational intensive tasks within \acrshort{ATLASlabel} and solutions involving adversarial networks and auto-encoders are being studied to output faster output, specially from the calorimeter simulation. Regarding particle reconstruction and identification, examples of implementations can be found within the $\tau$ identification %https://inspirehep.net/literature/1795210
 or $b$-tagging algorithms %https://cds.cern.ch/record/2718948
 . In physics analyses, the use of \acrshort{MLlabel} is already standarised to typically reconstruct or discriminate the signal process.

\section{Pipeline}

Machine Learning is a very broad umbrella term covering all kinds of algorithms which are not per
se optimised for a specific task but are flexible enough to adapt to different problem sets by tuning
(training) their parameter set.
ML requires besides the model itself also preparation and follow-up processing steps. In which
extent they are necessary always depends on the available data, the model and its later application.
Figure 7.1 shows such an example workflow (the single steps are explained in more detail in the
dedicated sections e.g. sec. 9.2).

Generally, two types of machine learning are distinguished: Supervised learning requiring fully
labelled training data and Unsupervised learning not requiring any labelled data. There are also

intermediate approaches called Semi-Supervised learning. In the context of this thesis supervised
approaches are used based on Neural Networks (NNs) and Boosted Decision Trees (BDTs).
In the following, a statistical parametric (ML) model is denoted as Pmodel(~xi; theta) parametrised with
the parameters theta while Pdata is the true but unknown distribution. A data set of length N is given
as X~ = (~x1,~x2, ...,~xN), in which each data point i has a feature set ~xi = (x
1
i
, x
2
i
, ..., xM
i
) with M
features and true labels yi in case of supervised learning.

t is important to ensure an unbiased training process. For this purpose, at least three orthogonal
datasets are needed as indicated in Figure 7.2. The training sample is utilised for the actual algorithm
training. The validation set is typically used to choose between different models and to optimise
the model further such as hyperparameter optimisation. While for the training itself mostly a loss
function is used (see sec. 7.2.3) to find the best parameter set, on the validation set, the performance
measures dedicated for the problem set are evaluated (e.g. signal over background ratio) to fine-tune
the model choice. The testing sample is only used to evaluate the final performance and is not involved in the training process. In the case of samples with low statistics, one can use cross-validation
or also called k-folding [139] where pairs of training and test-/validation sets are partitioned into k
subsets.

Typically, in particle physics the event number1 variable is used to split the dataset into the training
and testing set. The advantage is that at every point it is clear which events were used for the training
1 The event number is a unique integer number associated to each event not correlated with any physical observable.
\section{Performance}

Even though every ML application is different, the model performance is the decisive measure in the
end. Depending on the task, different metrics are used to judge the performance. In the following,
the most common approaches are discussed.

Likelihood Discriminant

Loss Function

\section{Neural networks}

Optimiser

Backpropagation

Activation Functions

Regularisation

\section{BDTs if used}